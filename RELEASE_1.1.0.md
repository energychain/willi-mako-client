# Release 1.1.0 - OpenAI-Compatible Chat Completions

**Release Date:** December 17, 2025

## üöÄ New Features

### OpenAI-Compatible Chat Completions API

Die neue Version 1.1.0 des Backend-APIs bringt einen OpenAI-kompatiblen `/chat/completions` Endpoint, der eine nahtlose Integration mit bestehenden OpenAI-basierten Workflows erm√∂glicht.

**Highlights:**

- ‚úÖ **Drop-in Replacement f√ºr OpenAI API** - Nur Base-URL und API-Key √§ndern
- ‚úÖ **Automatische RAG-Enhancement** - Immer aktive QDrant-Suche √ºber 5 Collections
- ‚úÖ **Stateless Operation** - Keine Session erforderlich (aber optional unterst√ºtzt)
- ‚úÖ **System Instructions Support** - via messages array (OpenAI-Format)
- ‚úÖ **Collection Targeting** - Einschr√§nkung auf spezifische Collections m√∂glich
- ‚úÖ **RAG Metadata Transparency** - Detaillierte Informationen √ºber Retrieval-Prozess

### Client-Implementierung

**TypeScript/JavaScript SDK:**

```typescript
import { WilliMakoClient } from 'willi-mako-client';

const client = new WilliMakoClient({
  token: process.env.WILLI_MAKO_TOKEN
});

const response = await client.createChatCompletion({
  messages: [
    { role: 'system', content: 'Du bist ein Experte f√ºr Marktkommunikation.' },
    { role: 'user', content: 'Was ist der Unterschied zwischen UTILMD und MSCONS?' }
  ],
  temperature: 0.7,
  max_tokens: 2048
});

console.log(response.choices[0].message.content);
console.log(`RAG docs: ${response.x_rag_metadata.retrieved_documents}`);
```

**CLI:**

```bash
willi-mako chat completions \
  --message "Was ist der Unterschied zwischen UTILMD und MSCONS?" \
  --system "Du bist ein Experte f√ºr Marktkommunikation." \
  --temperature 0.7 \
  --max-tokens 2048
```

**MCP Server:**

Neues Tool `create-chat-completion` verf√ºgbar f√ºr Model Context Protocol Integrationen.

### Python/OpenAI SDK Integration

```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_WILLI_MAKO_TOKEN",
    base_url="https://stromhaltig.de/api/v2"
)

response = client.chat.completions.create(
    model="willi-mako-rag",
    messages=[
        {"role": "user", "content": "Was ist der Unterschied zwischen UTILMD und MSCONS?"}
    ]
)

print(response.choices[0].message.content)
print(f"RAG Docs: {response.x_rag_metadata['retrieved_documents']}")
```

## üì¶ API Changes

### New Request Interface

```typescript
interface ChatCompletionRequest {
  messages: ChatCompletionMessage[];
  model?: string;
  temperature?: number;
  max_tokens?: number;
  top_p?: number;
  context_settings?: ChatCompletionContextSettings;
  session_id?: string;
}
```

### New Response Interface

```typescript
interface ChatCompletionResponse {
  id: string;
  object: 'chat.completion';
  created: number;
  model: string;
  choices: ChatCompletionChoice[];
  usage: ChatCompletionUsage;
  x_rag_metadata: ChatCompletionRagMetadata;
  x_system_info: ChatCompletionSystemInfo;
}
```

## üîß Technical Details

### RAG-Prozess (immer aktiv)

```
Client Request (OpenAI Format)
        ‚Üì
[/api/v2/chat/completions]
        ‚Üì
1) Parse OpenAI-Format
   - Extract messages
   - Extract system instructions
   - Merge context_settings
        ‚Üì
2) RAG Context Retrieval (IMMER AKTIV!)
   - Semantic search in QDrant
   - Collections: targetCollections oder DEFAULT: alle 5
        ‚Üì
3) Context Merging
   - RAG-Results (prim√§r)
   - User documents (wenn enabled)
   - System instructions
   - Conversation history
        ‚Üì
4) LLM Generation
   - Enriched Context ‚Üí LLM
   - Apply temperature, max_tokens
        ‚Üì
5) Format Response (OpenAI-kompatibel)
   + x_rag_metadata Extension
        ‚Üì
Client Response (OpenAI Format + Extensions)
```

## üìö Documentation Updates

- Neue Beispiel-Datei: `examples/openai-compatible-chat.ts`
- CLI-Dokumentation erweitert: `willi-mako chat completions --help`
- MCP Server Instructions aktualisiert
- README mit OpenAI-Kompatibilit√§ts-Sektion

## üîÑ Migration Guide

### Von OpenAI zu Willi-Mako

**Vorher (OpenAI):**

```typescript
const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});
```

**Nachher (Willi-Mako):**

```typescript
const client = new OpenAI({
  apiKey: process.env.WILLI_MAKO_TOKEN,
  baseUrl: 'https://stromhaltig.de/api/v2'
});
```

**Das war's!** Der Rest des Codes bleibt identisch.

## üéØ Use Cases

1. **OpenAI Migration** - Bestehende OpenAI-Integration ohne Code-√Ñnderungen nutzen
2. **Externe Tools** - Jedes Tool, das OpenAI SDK nutzt, funktioniert automatisch
3. **Stateless Requests** - Kein Session-Management f√ºr einfache Anfragen
4. **Custom System Instructions** - Projekt-spezifische Anweisungen pro Request
5. **Collection Targeting** - Fokussierung auf relevante Wissensbereiche

## üêõ Bug Fixes

- OpenAPI Schema auf Version 1.1.0 aktualisiert
- TypeScript Types f√ºr neue Endpoints hinzugef√ºgt

## üîó Links

- Backend API Dokumentation: https://stromhaltig.de/api/v2/openapi.json
- Client Repository: https://github.com/energychain/willi-mako-client
- NPM Package: https://www.npmjs.com/package/willi-mako-client

## üôè Credits

Entwickelt f√ºr die deutsche Energiewirtschaft mit Fokus auf Marktkommunikation (GPKE, WiM, GeLi Gas), EDIFACT/edi@energy Standards, und Regulierung (EnWG, BNetzA).

---

**Next Steps:**
- Streaming Support f√ºr Chat Completions (Phase 2)
- Function Calling Support (zuk√ºnftig)
- Enhanced RAG Metrics und Explanability
